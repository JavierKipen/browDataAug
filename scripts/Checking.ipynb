{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92bc2f83-2bf3-4108-acdc-3428f32b91b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# https://stackoverflow.com/questions/66828031/do-i-always-have-to-restart-my-kernel-in-jupyter-lab-when-code-in-a-local-module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c519edef-ca89-4590-9ca0-9bbe8f3df005",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.set_visible_devices(physical_devices[0], 'GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f542017-92b5-4b0b-a528-10a679b44607",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Dataset Generation\n",
    "The functions to manipulate the dataset allow for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "aee4c905-a782-4f36-876b-24689f93ba43",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Train  Test\n",
      "000   5769   315\n",
      "001   7665  1340\n",
      "010   2418   101\n",
      "011  14231  2378\n",
      "100    919    83\n",
      "101   7551   427\n",
      "110   6731   606\n",
      "111   6979   665\n",
      "         Train       Test\n",
      "000  94.822485   5.177515\n",
      "001  85.119378  14.880622\n",
      "010  95.990472   4.009528\n",
      "011  85.682461  14.317539\n",
      "100  91.716567   8.283433\n",
      "101  94.647781   5.352219\n",
      "110  91.740493   8.259507\n",
      "111  91.300366   8.699634\n"
     ]
    }
   ],
   "source": [
    "from DatasetFuncs import allDataset_loader,dataset_split,show_porcentages,show_partition_nanopores\n",
    "\n",
    "data_folder=\"../ext/QuipuData/\"; #path where the datasets are/will be stored\n",
    "allDatasets=allDataset_loader(data_folder) #Generates the processed df and saves it. If it is already in memory\n",
    "#then it loads it. One can specify cut=False to obtain the uncut traces (if not they are cut at 700 samples as was used in quipunet)\n",
    "\n",
    "trainSet,testSet=dataset_split(allDatasets)#Divides in train and test, in a way that the test set is within a range of\n",
    "# % of samples, and that nanopores used on train set are not used in train for the same barcode.\n",
    "samples_perc=show_porcentages(trainSet,testSet) #This verifies the percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bc3b6fe0-8a72-4d2a-bc14-453f7bb8d007",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   barcode  nanopore  size\n",
      "0      000         7  2172\n",
      "1      000         8  3498\n",
      "2      000      1017    99\n",
      "3      001         8  1737\n",
      "4      001        27  3583\n",
      "5      001        28   923\n",
      "6      001        29  1255\n",
      "7      001      1053   167\n",
      "8      010         7  1021\n",
      "9      010         9   629\n",
      "10     010        10   768\n",
      "11     011        11   362\n",
      "12     011        12   577\n",
      "13     011        31   899\n",
      "14     011        33  2192\n",
      "15     011        35   483\n",
      "16     011        36  1780\n",
      "17     011        37  1583\n",
      "18     011        38  1229\n",
      "19     011        39  1735\n",
      "20     011        40  2120\n",
      "21     011        41  1271\n",
      "22     100         7   288\n",
      "23     100        13   631\n",
      "24     101         8  1240\n",
      "25     101        26  1764\n",
      "26     101        27   924\n",
      "27     101        29  3494\n",
      "28     101      1662   129\n",
      "29     110         7  1161\n",
      "30     110         8  2702\n",
      "31     110        13  1309\n",
      "32     110        14  1559\n",
      "33     111         7   735\n",
      "34     111         8  2866\n",
      "35     111         9   677\n",
      "36     111        15  1092\n",
      "37     111        32   346\n",
      "38     111        34  1263\n",
      "   barcode  nanopore  size\n",
      "0      000         6   253\n",
      "1      000      1014    62\n",
      "2      001         7   838\n",
      "3      001        26   502\n",
      "4      010      1159   101\n",
      "5      011         6  1332\n",
      "6      011         7   702\n",
      "7      011        32   344\n",
      "8      100      1933    83\n",
      "9      101        30   427\n",
      "10     110        12   606\n",
      "11     111        14   665\n"
     ]
    }
   ],
   "source": [
    "#With this code we show which is the nanopores used for this train and test dataset\n",
    "print(show_partition_nanopores(trainSet))\n",
    "print(show_partition_nanopores(testSet))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a28a9c-e140-4e85-8672-06973e0939c0",
   "metadata": {},
   "source": [
    "# Testing reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a73b1c-b19a-4a9e-b9bb-31c1d570d94c",
   "metadata": {},
   "source": [
    "With this function we can run the training of quipu how it was done in their notebook (with n epochs=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad775c05-80d0-4aad-9bbd-21b7cae39761",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jkipen/anaconda3/envs/browDataAug/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Epoch: 0 ===\n",
      "  prep time: 3.0 sec   train time: 21.0 sec\n",
      "  loss: 1.893   acc: 0.2963   val_acc: 0.4077\n",
      "=== Epoch: 1 ===\n",
      "  prep time: 3.0 sec   train time: 17.0 sec\n",
      "  loss: 1.694   acc: 0.3553   val_acc: 0.4316\n",
      "       [ loss , accuracy ]\n",
      "Train: [1.465266466140747, 0.4333359897136688]\n",
      "Validation  : [1.4687469005584717, 0.43164435029029846]\n",
      "Test : [1.507033109664917, 0.49643704295158386]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.4333359897136688, 0.43164435029029846, 0.49643704295158386)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ModelTrainer import ModelTrainer\n",
    "mt=ModelTrainer()\n",
    "mt.quipu_def_train(n_epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2161aa8d-725d-4ca1-8c11-e728f17e1f29",
   "metadata": {},
   "source": [
    "Then we did this for 20 runs and saved it in \"../results/QuipuReproduction.csv\", so when we load the datafram we can observe the accuracies and their variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35f1bf26-be37-44b4-8683-0ecb9887be7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_accs(array_accs): #Array that has 3 columns, train valid and test accs, each row is a model result\n",
    "    all_means=np.mean(all_accs,axis=0)\n",
    "    all_stds=np.std(all_accs,axis=0)\n",
    "    print(\"Train Acc: \" + \"{:.2f}\".format(all_means[0]*100) + \" +- \" + \"{:.2f}\".format(all_stds[0]*100) + \" % \")\n",
    "    print(\"Valid Acc: \" + \"{:.2f}\".format(all_means[1]*100) + \" +- \" + \"{:.2f}\".format(all_stds[1]*100) + \" % \")\n",
    "    print(\"Test Acc: \" + \"{:.2f}\".format(all_means[2]*100) + \" +- \" + \"{:.2f}\".format(all_stds[2]*100) + \" % \")\n",
    "    n_samples=np.shape(array_accs)[0]\n",
    "    print(\"Test Accuracy estimator: \" + \"{:.2f}\".format(all_means[2]*100) + \" +- \" + \"{:.2f}\".format(all_stds[2]*100/np.sqrt(n_samples)) + \" % \")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0948af7-6fee-44df-8c51-67b1f7ba9d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc: 97.65 +- 0.27 % \n",
      "Valid Acc: 96.20 +- 0.48 % \n",
      "Test Acc: 92.84 +- 1.47 % \n",
      "Test Accuracy estimator: 92.84 +- 0.33 % \n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"../results/QuipuReproduction.csv\")\n",
    "all_accs=df.iloc[:,:-1].values\n",
    "print_accs(all_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97745dbb-dedb-4f9f-98b4-5698c70de400",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9513720571994781"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(df[\"Test Acc\"].values[np.argsort(df[\"Test Acc\"].values)[-2:]]) #Top 10% test accuracy (2 samples is 10% of 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ebcebcf-1f83-4e13-a2d0-57a2ba8093d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc: 98.90 +- 0.17 % \n",
      "Valid Acc: 96.53 +- 0.24 % \n",
      "Test Acc: 92.58 +- 1.77 % \n",
      "Test Accuracy estimator: 92.58 +- 0.40 % \n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"../results/QuipuTrainedWithES.csv\")\n",
    "all_accs=df.iloc[:,:-1].values\n",
    "print_accs(all_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f47f9b1-e05d-47f9-bef6-5b132a5d7da0",
   "metadata": {},
   "source": [
    "## To check if we can remove it. Old used to debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f52dfd44-b1f9-4114-b846-df51cd8604b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(all_accs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77a4f544-e47a-47df-8fb9-0c5b827b2b85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "<class 'TypeError'>",
     "evalue": "'tuple' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m X_train,X_valid,Y_train,Y_valid,X_test,Y_test\u001b[38;5;241m=\u001b[39mdl\u001b[38;5;241m.\u001b[39mget_datasets_numpy();\n\u001b[1;32m      6\u001b[0m da\u001b[38;5;241m=\u001b[39mDataAugmentator();\n\u001b[0;32m----> 7\u001b[0m out\u001b[38;5;241m=\u001b[39mda\u001b[38;5;241m.\u001b[39mtest_brow_aug(X_train)\n",
      "File \u001b[0;32m~/browDataAug/scripts/DataAugmentator.py:46\u001b[0m, in \u001b[0;36mDataAugmentator.test_brow_aug\u001b[0;34m(self, X_train)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest_brow_aug\u001b[39m(\u001b[38;5;28mself\u001b[39m,X_train):\n\u001b[1;32m     45\u001b[0m     X \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(X_train) \u001b[38;5;66;03m# make copies\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m     noise\u001b[38;5;241m=\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbrow_std;\n\u001b[1;32m     47\u001b[0m     data_out,ev_len_out\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbrowAug(data_in\u001b[38;5;241m=\u001b[39mev_data_array,noise\u001b[38;5;241m=\u001b[39mnoise)\n\u001b[1;32m     48\u001b[0m     data_out\u001b[38;5;241m=\u001b[39mdata_out\u001b[38;5;241m.\u001b[39mnumpy();\n",
      "File \u001b[0;32mmtrand.pyx:1270\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.randn\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mmtrand.pyx:1431\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.standard_normal\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_common.pyx:636\u001b[0m, in \u001b[0;36mnumpy.random._common.cont\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "from DataLoader import DataLoader\n",
    "from DataAugmentator import DataAugmentator\n",
    "dl=DataLoader();\n",
    "#X_train,X_valid,Y_train,Y_valid,X_test,Y_test=dl.get_datasets_numpy_quipu();\n",
    "X_train,X_valid,Y_train,Y_valid,X_test,Y_test=dl.get_datasets_numpy();\n",
    "da=DataAugmentator();\n",
    "out=da.test_brow_aug(X_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e06962-9d4c-405c-98c6-90e21c1c7a49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (XPython)",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
